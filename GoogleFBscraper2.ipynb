{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleFBscraper2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hrXqLnQIB2TVY5-B04kCCSfrp_ozFXuH",
      "authorship_tag": "ABX9TyO4H/BQ6IW0ElCpPppP+52Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bilal-Subhopoto/DataEngineering-Bilal/blob/main/GoogleFBscraper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tldextract "
      ],
      "metadata": {
        "id": "JEnndiin_YKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facebook_scraper"
      ],
      "metadata": {
        "id": "qTK-vsLo7Y8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install redis"
      ],
      "metadata": {
        "id": "lKX-78Cm8UZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googlesearch import search\n",
        "import tldextract\n",
        "import time\n",
        "import random\n",
        "import redis\n",
        "from facebook_scraper import get_posts\n",
        "masterDB = redis.Redis(host='43.251.253.107', port=1500, db=0)\n",
        "DB1 = redis.Redis(host='43.251.253.107', port=1500, db=1)\n",
        "DB2 = redis.Redis(host='43.251.253.107', port=1500, db=2)\n",
        "import pymongo\n",
        "myclient = pymongo.MongoClient(\"mongodb://43.251.253.107:1600/\")\n",
        "mydb = myclient['GoogleFBscraper2-Bilal']\n",
        "mycol = mydb['Allposts']"
      ],
      "metadata": {
        "id": "V9SO1y9qqhex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read sets' keys from redis masterDB and decode\n",
        "sitenames_list = []\n",
        "for keyb in masterDB.scan_iter():\n",
        "  print(keyb)\n",
        "  keyS = keyb.decode(\"utf-8\")\n",
        "  sitenames_list.append(keyS)# read all the keys from redis into python list\n",
        "  # masterDB.spop(key1)# loop over the above list and delete each entry from redis\n",
        "print(sitenames_list)#simple strings inside list, NOT byte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qXRd3far52M",
        "outputId": "7bc347b6-cb2e-47f7-90c6-6c3dc8de3524"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Time'\n",
            "b'Telegraph'\n",
            "b'Business Insider'\n",
            "b'Boston.com'\n",
            "b'Boston Globe'\n",
            "b'Fox News Digital Network'\n",
            "b'New York Daily News'\n",
            "b'CBS News'\n",
            "b'Huffington Post'\n",
            "b'Bleacher Report'\n",
            "b'MSN News'\n",
            "b'Detroit Free Press'\n",
            "b'Los Angeles Times'\n",
            "b'Examiner'\n",
            "b'Atlantic'\n",
            "b'Slate'\n",
            "b'Chron'\n",
            "b'Salon'\n",
            "b'Mic'\n",
            "b'BBC'\n",
            "b'CNET'\n",
            "b'New York Post'\n",
            "b'Washington Post'\n",
            "b'Vice'\n",
            "b'Yahoo-ABC News Network'\n",
            "b'NPR'\n",
            "b'New York Times'\n",
            "b'U.S. News'\n",
            "b'Upworthy'\n",
            "b'AL.com'\n",
            "b'Chicago Tribune'\n",
            "b'NBC News'\n",
            "b'Mashable'\n",
            "b'TechCrunch'\n",
            "b'Engadget'\n",
            "b'USAToday'\n",
            "b'SFGate'\n",
            "b'TheBlaze'\n",
            "b'BuzzFeed'\n",
            "b'Daily Mail'\n",
            "b'Independent'\n",
            "b'Guardian'\n",
            "b'Dallas Morning News'\n",
            "b'MLive'\n",
            "b'Mirror Online'\n",
            "b'Daily Beast'\n",
            "b'Elite Daily'\n",
            "b'CNN'\n",
            "['Time', 'Telegraph', 'Business Insider', 'Boston.com', 'Boston Globe', 'Fox News Digital Network', 'New York Daily News', 'CBS News', 'Huffington Post', 'Bleacher Report', 'MSN News', 'Detroit Free Press', 'Los Angeles Times', 'Examiner', 'Atlantic', 'Slate', 'Chron', 'Salon', 'Mic', 'BBC', 'CNET', 'New York Post', 'Washington Post', 'Vice', 'Yahoo-ABC News Network', 'NPR', 'New York Times', 'U.S. News', 'Upworthy', 'AL.com', 'Chicago Tribune', 'NBC News', 'Mashable', 'TechCrunch', 'Engadget', 'USAToday', 'SFGate', 'TheBlaze', 'BuzzFeed', 'Daily Mail', 'Independent', 'Guardian', 'Dallas Morning News', 'MLive', 'Mirror Online', 'Daily Beast', 'Elite Daily', 'CNN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google-search each sitename from the above list\n",
        "urls = []\n",
        "for ttls in sitenames_list:\n",
        "  \n",
        "  try:\n",
        "    resp=search(\"facebook.com \"+ttls, num=2)   \n",
        "    time.sleep(random.randint(5,12))\n",
        "    for link in resp:\n",
        "      childFLD=tldextract.extract(link)\n",
        "      childFLD=childFLD.domain\n",
        "      if childFLD==\"facebook\":\n",
        "        try:\n",
        "          print(link)\n",
        "          pageNames = link.split('/')[3].strip()\n",
        "          # pageNames = link.split('/')[2:3]use either\n",
        "          print(pageNames)\n",
        "          DB1.sadd(pageNames,'sitename') # insert the extracted unique fb page name into redis DB-1\n",
        "          masterDB.spop(ttls) # in the for loop of list from redis after finding unique page name on google delete that particular key of sitename from DB-0\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          break\n",
        "        break\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break\n",
        "# make sure that the key(s) is deleted from DB-0 only when a succesful unique Fb page name is found.\n",
        "# make sure when google blocks us the program should enddddd\n",
        "# this concept is state management in DB-0 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o3TV4ApsKDH",
        "outputId": "c979c2c8-aa4a-491e-9280-2462d31fc6ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.facebook.com/time/\n",
            "time\n",
            "https://www.facebook.com/TELEGRAPH.CO.UK/\n",
            "TELEGRAPH.CO.UK\n",
            "https://www.facebook.com/businessinsider/\n",
            "businessinsider\n",
            "https://www.facebook.com/boston/\n",
            "boston\n",
            "https://www.facebook.com/globe/\n",
            "globe\n",
            "https://www.facebook.com/FoxNews\n",
            "FoxNews\n",
            "https://www.facebook.com/NYDailyNews/\n",
            "NYDailyNews\n",
            "https://www.facebook.com/CBSNews/\n",
            "CBSNews\n",
            "https://www.facebook.com/HuffPost/\n",
            "HuffPost\n",
            "https://www.facebook.com/bleacherreport/\n",
            "bleacherreport\n",
            "https://www.facebook.com/msn/\n",
            "msn\n",
            "https://www.facebook.com/detroitfreepress/\n",
            "detroitfreepress\n",
            "https://www.facebook.com/latimes/\n",
            "latimes\n",
            "https://www.facebook.com/examiner.net/\n",
            "examiner.net\n",
            "https://www.facebook.com/TheAtlantic/\n",
            "TheAtlantic\n",
            "https://www.facebook.com/Slate/\n",
            "Slate\n",
            "https://www.facebook.com/chroncom/\n",
            "chroncom\n",
            "HTTP Error 429: Too Many Requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read sets' keys from redis DB1 and decode\n",
        "for keyb1 in DB1.scan_iter():\n",
        "  print(keyb1)\n",
        "  keyS1 = keyb1.decode(\"utf-8\")\n",
        "  urls.append(keyS1)# read all the keys from redis into python list\n",
        "  # masterDB.spop(key1)# loop over the above list and delete each entry from redis\n",
        "print(urls)#simple strings inside list, NOT byte from DB1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7eHjCkJ-sp",
        "outputId": "5a2a7259-4291-46e8-fe29-9e05fbc8fd6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'msn'\n",
            "b'Vox'\n",
            "b'bleacherreport'\n",
            "b'chroncom'\n",
            "b'boston'\n",
            "b'latimes'\n",
            "b'examiner.net'\n",
            "b'TELEGRAPH.CO.UK'\n",
            "b'detroitfreepress'\n",
            "b'NYDailyNews'\n",
            "b'HuffPost'\n",
            "b'Gawker'\n",
            "b'NJ.com'\n",
            "b'Slate'\n",
            "b'businessinsider'\n",
            "b'TheAtlantic'\n",
            "b'CBSNews'\n",
            "b'FoxNews'\n",
            "b'time'\n",
            "b'globe'\n",
            "['msn', 'Vox', 'bleacherreport', 'chroncom', 'boston', 'latimes', 'examiner.net', 'TELEGRAPH.CO.UK', 'detroitfreepress', 'NYDailyNews', 'HuffPost', 'Gawker', 'NJ.com', 'Slate', 'businessinsider', 'TheAtlantic', 'CBSNews', 'FoxNews', 'time', 'globe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fb urls to extract posts data and upload on DB-1\n",
        "fields = ('comments','link','likes','images','shared_time','text','post_id','reactions','shares','user_id','time','timestamp','time')\n",
        "# fbPages = ('ABCNews','cnn','NBCNews','mlive','TheIndependentOnline','thedailybeast','WashingtonExaminer','Gawker','Upworthy','chroncom','Slate','bleacherreport','EliteDaily','washingtonpost','DailyMail','nytimes','BuzzFeed','usatoday','CBSNews','HuffPost')\n",
        "for fbp in urls:\n",
        "  try:\n",
        "    for post in get_posts(fbp, pages= 3,cookies= 'cookies.txt'):\n",
        "      collection = {'pageName':fbp}    \n",
        "      for items in fields:\n",
        "        collection[items] = (post[items])\n",
        "        # DB2.hset(fbp,items,post[items])\n",
        "      x = mycol.insert_one(collection) \n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "AB-GMTpdGBz6"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}